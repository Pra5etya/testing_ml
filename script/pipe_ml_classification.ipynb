{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# data preparation\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "# data modeling\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# data scoring\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# data tuning   \n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_round(x, pos): \n",
    "    if abs(x) >= 1e9: \n",
    "        return f'{x/1e9} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x}'\n",
    "    \n",
    "def val_round(x):\n",
    "    if abs(x) >= 1e9:\n",
    "        return f'{x/1e9:.2f} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6:.2f} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3:.2f} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers_iqr(df, columns = None, threshold = 1.5):\n",
    "    # Jika tidak ada kolom yang ditentukan, gunakan semua kolom numerik\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include = [\"number\"]).columns.tolist()\n",
    "    \n",
    "    # Salin DataFrame untuk memastikan tidak ada modifikasi langsung\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        # Hitung Q1, Q3, dan IQR\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Hitung batas bawah dan atas\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        # Hapus baris dengan outlier\n",
    "        df_filtered = df_filtered[(df_filtered[column] >= lower_bound) & (df_filtered[column] <= upper_bound)]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk konversi tipe data\n",
    "def convert_object_columns_to_numeric(df):\n",
    "    for col in df.select_dtypes(include = ['object']).columns:  \n",
    "        try:\n",
    "            # Cek apakah semua nilai bisa dikonversi ke float\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "            \n",
    "            # Jika bisa, ubah ke int jika semua nilai adalah bilangan bulat\n",
    "            if all(df[col] % 1 == 0):  # Cek apakah semua nilai adalah bilangan bulat\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "        except ValueError:\n",
    "            pass  # Jika ada nilai non-angka, biarkan tetap object\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat data train dan test\n",
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all column\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "train_df = train_df.drop('Id', axis = 1)\n",
    "\n",
    "# convert object if all numeric\n",
    "train_df = convert_object_columns_to_numeric(train_df)\n",
    "\n",
    "# check duplicate general data\n",
    "print(f'Total General Duplicated: {train_df.duplicated().sum()} \\n')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null column\n",
    "null_numeric = []\n",
    "null_obj = []\n",
    "\n",
    "# \n",
    "null_columns = train_df.columns[train_df.isnull().sum() > 0]\n",
    "\n",
    "for col in null_columns:\n",
    "    if train_df[col].dtype in ['int', 'float']:\n",
    "        null_numeric.append(col)\n",
    "        \n",
    "    elif train_df[col].dtype == 'object':\n",
    "        null_obj.append(col)\n",
    "\n",
    "# \n",
    "print(\"Null Numeric:\", null_numeric)\n",
    "print(\"Null String:\", null_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "num_cols = []\n",
    "obj_cols = []\n",
    "\n",
    "for col in train_df:\n",
    "    if train_df[col].dtype in ['int', 'float']:\n",
    "        num_cols.append(col)\n",
    "        \n",
    "    elif train_df[col].dtype == 'object':\n",
    "        obj_cols.append(col)\n",
    "\n",
    "# \n",
    "print(\"Numeric Cols:\", num_cols)\n",
    "print(\"String Cols:\", obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns\n",
    "train_original = train_df.columns\n",
    "\n",
    "# Numeric Pipeline\n",
    "numerical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"mean\"))\n",
    "])\n",
    "\n",
    "# String Pipeline\n",
    "categorical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"most_frequent\"))\n",
    "])\n",
    "\n",
    "# ColumnTransformer untuk menggabungkan proses imputasi\n",
    "prep_stage_1 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_pipeline, num_cols), \n",
    "        (\"cat\", categorical_pipeline, obj_cols), \n",
    "    ], \n",
    "    remainder = \"drop\", \n",
    "    verbose_feature_names_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data menggunakan fit_transform pada tahap 1\n",
    "train_df = prep_stage_1.fit_transform(train_df)\n",
    "\n",
    "# Columns After: ubah kembali ke DataFrame dengan kolom dari prep_stage_1\n",
    "train_df = pd.DataFrame(train_df, columns = prep_stage_1.get_feature_names_out())\n",
    "\n",
    "# Hilangkan prefix (misalnya, \"num__\", \"cat__\", \"out__\")\n",
    "clean_columns = [col.split(\"__\", 1)[-1] for col in train_df.columns]\n",
    "train_df.columns = clean_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = train_df.isnull().sum()[train_df.isnull().sum() > 0]\n",
    "print(f'Total null columns: {null_columns} \\n')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change object after transform\n",
    "train_df = convert_object_columns_to_numeric(train_df)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cetak jumlah baris sebelum filter\n",
    "print(f\"Total Rows Before Filtering: {len(train_df)}\")\n",
    "\n",
    "# Pilih kolom numerik\n",
    "num_cols = train_df.select_dtypes(include = [\"number\"]).columns\n",
    "\n",
    "# Terapkan filter pada kolom numerik\n",
    "train_df = filter_outliers_iqr(train_df, columns = num_cols)\n",
    "\n",
    "# Cetak jumlah baris setelah filter\n",
    "print(f\"Total Rows After Filtering: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kolom untuk label encoding (kolom ordinal)\n",
    "encoding_set = {'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', \n",
    "                'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', \n",
    "                'FireplaceQu', 'GarageQual', 'GarageCond'}\n",
    "\n",
    "# Inisialisasi list untuk menyimpan kolom yang telah dikelompokkan\n",
    "train_ordinal_cols = []\n",
    "train_one_hot_cols = []\n",
    "train_numeric_cols = []\n",
    "\n",
    "# Mengelompokkan kolom berdasarkan tipe data\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype in ['int', 'float']:\n",
    "        train_numeric_cols.append(col)\n",
    "\n",
    "    elif train_df[col].dtype == 'object':\n",
    "        if col in encoding_set:\n",
    "            train_ordinal_cols.append(col)\n",
    "\n",
    "        else:\n",
    "            train_one_hot_cols.append(col)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Ordinal Encoding Columns:\", train_ordinal_cols)\n",
    "print(\"One-Hot Encoding Columns:\", train_one_hot_cols)\n",
    "print(\"Numeric Columns:\", train_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "test_df = test_df.drop('Id', axis = 1)\n",
    "\n",
    "# convert object if all numeric\n",
    "test_df = convert_object_columns_to_numeric(test_df)\n",
    "\n",
    "# check duplicate general data\n",
    "print(f'Total General Duplicated: {test_df.duplicated().sum()} \\n')\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null column\n",
    "null_numeric = []\n",
    "null_obj = []\n",
    "\n",
    "# \n",
    "null_columns = test_df.columns[test_df.isnull().sum() > 0]\n",
    "\n",
    "for col in null_columns:\n",
    "    if test_df[col].dtype in ['int', 'float']:\n",
    "        null_numeric.append(col)\n",
    "        \n",
    "    elif test_df[col].dtype == 'object':\n",
    "        null_obj.append(col)\n",
    "\n",
    "# \n",
    "print(\"Null Numeric:\", null_numeric)\n",
    "print(\"Null String:\", null_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "num_cols = []\n",
    "obj_cols = []\n",
    "\n",
    "for col in test_df:\n",
    "    if test_df[col].dtype in ['int', 'float']:\n",
    "        num_cols.append(col)\n",
    "        \n",
    "    elif test_df[col].dtype == 'object':\n",
    "        obj_cols.append(col)\n",
    "\n",
    "# \n",
    "print(\"Numeric Cols:\", num_cols)\n",
    "print(\"String Cols:\", obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns\n",
    "test_original = test_df.columns\n",
    "\n",
    "# Numeric Pipeline\n",
    "numerical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"mean\"))\n",
    "])\n",
    "\n",
    "# String Pipeline\n",
    "categorical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"most_frequent\"))\n",
    "])\n",
    "\n",
    "# ColumnTransformer untuk menggabungkan proses imputasi\n",
    "prep_stage_1 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_pipeline, num_cols), \n",
    "        (\"cat\", categorical_pipeline, obj_cols), \n",
    "    ], \n",
    "    remainder = \"drop\", \n",
    "    verbose_feature_names_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data menggunakan fit_transform pada tahap 1\n",
    "test_df = prep_stage_1.fit_transform(test_df)\n",
    "\n",
    "# Columns After: ubah kembali ke DataFrame dengan kolom dari prep_stage_1\n",
    "test_df = pd.DataFrame(test_df, columns = prep_stage_1.get_feature_names_out())\n",
    "\n",
    "# Hilangkan prefix (misalnya, \"num__\", \"cat__\", \"out__\")\n",
    "clean_columns = [col.split(\"__\", 1)[-1] for col in test_df.columns]\n",
    "test_df.columns = clean_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = test_df.isnull().sum()[test_df.isnull().sum() > 0]\n",
    "print(f'Total null columns: {null_columns} \\n')\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change object after transform\n",
    "test_df = convert_object_columns_to_numeric(test_df)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cetak jumlah baris sebelum filter\n",
    "print(f\"Total Rows Before Filtering: {len(test_df)}\")\n",
    "\n",
    "# Pilih kolom numerik\n",
    "num_cols = test_df.select_dtypes(include = [\"number\"]).columns\n",
    "\n",
    "# Terapkan filter pada kolom numerik\n",
    "test_df = filter_outliers_iqr(test_df, columns = num_cols)\n",
    "\n",
    "# Cetak jumlah baris setelah filter\n",
    "print(f\"Total Rows After Filtering: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kolom untuk label encoding (kolom ordinal)\n",
    "encoding_set = {'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', \n",
    "                'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', \n",
    "                'FireplaceQu', 'GarageQual', 'GarageCond'}\n",
    "\n",
    "# Inisialisasi list untuk menyimpan kolom yang telah dikelompokkan\n",
    "test_ordinal_cols = []\n",
    "test_one_hot_cols = []\n",
    "test_numeric_cols = []\n",
    "\n",
    "# Mengelompokkan kolom berdasarkan tipe data\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtype in ['int', 'float']:\n",
    "        test_numeric_cols.append(col)\n",
    "\n",
    "    elif test_df[col].dtype == 'object':\n",
    "        if col in encoding_set:\n",
    "            test_ordinal_cols.append(col)\n",
    "\n",
    "        else:\n",
    "            test_one_hot_cols.append(col)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Ordinal Encoding Columns:\", test_ordinal_cols)\n",
    "print(\"One-Hot Encoding Columns:\", test_one_hot_cols)\n",
    "print(\"Numeric Columns:\", test_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifikasi kolom-kolom yang ada di train dan test\n",
    "ordinal_encoding_cols = list(set(train_ordinal_cols) & set(test_ordinal_cols))\n",
    "one_hot_encoding_cols = list(set(train_one_hot_cols) & set(test_one_hot_cols))\n",
    "numeric_cols = list(set(train_numeric_cols) & set(test_numeric_cols))\n",
    "\n",
    "# \n",
    "print(f'ordinal cols: {ordinal_encoding_cols}')\n",
    "print(f'one-hot cols: {one_hot_encoding_cols}')\n",
    "print(f'numeric cols: {numeric_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)\n",
    "ordinal_transformer = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)\n",
    "\n",
    "prep_stage_2 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_transformer, numeric_cols), \n",
    "        (\"cat\", categorical_transformer, one_hot_encoding_cols), \n",
    "        (\"ord\", ordinal_transformer, ordinal_encoding_cols)\n",
    "    ], remainder = \"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "transformed_data = prep_stage_2.fit_transform(train_df)\n",
    "\n",
    "# Membuat DataFrame dengan kolom hasil transformasi\n",
    "# ====================================================\n",
    "# Mendapatkan nama kolom baru untuk OneHotEncoder\n",
    "categorical_feature_names = prep_stage_2.named_transformers_[\"cat\"].get_feature_names_out(one_hot_encoding_cols)\n",
    "\n",
    "# Gabungkan semua nama kolom\n",
    "all_columns = (\n",
    "    numeric_cols +\n",
    "    list(categorical_feature_names) +\n",
    "    ordinal_encoding_cols +\n",
    "    list(train_df.columns.difference(numeric_cols + one_hot_encoding_cols + ordinal_encoding_cols))\n",
    ")\n",
    "\n",
    "# Membuat DataFrame dengan nama kolom yang sesuai\n",
    "train_df = pd.DataFrame(transformed_data, columns = all_columns)\n",
    "\n",
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = train_df.isnull().sum()[train_df.isnull().sum() > 0]\n",
    "print(f'Train Stage 2 Check: {null_columns}')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "transformed_data = prep_stage_2.fit_transform(test_df)\n",
    "\n",
    "# Membuat DataFrame dengan kolom hasil transformasi\n",
    "# ====================================================\n",
    "# Mendapatkan nama kolom baru untuk OneHotEncoder\n",
    "categorical_feature_names = prep_stage_2.named_transformers_[\"cat\"].get_feature_names_out(one_hot_encoding_cols)\n",
    "\n",
    "# Gabungkan semua nama kolom\n",
    "all_columns = (\n",
    "    numeric_cols +\n",
    "    list(categorical_feature_names) +\n",
    "    ordinal_encoding_cols +\n",
    "    list(test_df.columns.difference(numeric_cols + one_hot_encoding_cols + ordinal_encoding_cols))\n",
    ")\n",
    "\n",
    "# Membuat DataFrame dengan nama kolom yang sesuai\n",
    "test_df = pd.DataFrame(transformed_data, columns = all_columns)\n",
    "\n",
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = test_df.isnull().sum()[test_df.isnull().sum() > 0]\n",
    "print(f'Test Stage 2 Check: {null_columns}')\n",
    "test_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan kolom target dari data\n",
    "# Mencari kolom target di train_df\n",
    "train_target_col = train_df.filter(like='MSZoning_').columns\n",
    "\n",
    "if len(train_target_col) > 0:\n",
    "    train_target_col = train_target_col[0]  # Mengambil kolom pertama yang cocok\n",
    "    \n",
    "    # Pastikan kolom target ada di train_df\n",
    "    if train_target_col in train_df.columns:\n",
    "        X_train = train_df.drop(columns=[train_target_col])\n",
    "        y_train = train_df[train_target_col]\n",
    "    else:\n",
    "        raise ValueError(f\"Kolom target '{train_target_col}' tidak ditemukan di train_df.\")\n",
    "else:\n",
    "    raise ValueError(\"Kolom dengan filter 'MSZoning_' tidak ditemukan di train_df.\")\n",
    "\n",
    "# Mencari kolom target di test_df\n",
    "test_target_col = test_df.filter(like='MSZoning_').columns\n",
    "\n",
    "if len(test_target_col) > 0:\n",
    "    test_target_col = test_target_col[0]  # Mengambil kolom pertama yang cocok\n",
    "    \n",
    "    # Pastikan kolom target ada di test_df\n",
    "    if test_target_col in test_df.columns:\n",
    "        X_test = test_df.drop(columns=[test_target_col])\n",
    "        y_test = test_df[test_target_col]\n",
    "    else:\n",
    "        raise ValueError(f\"Kolom target '{test_target_col}' tidak ditemukan di test_df.\")\n",
    "else:\n",
    "    raise ValueError(\"Kolom dengan filter 'MSZoning_' tidak ditemukan di test_df.\")\n",
    "\n",
    "# Validasi tambahan\n",
    "print(\"Shape X_train:\", X_train.shape)\n",
    "print(\"Shape y_train:\", y_train.shape)\n",
    "print(\"Shape X_test:\", X_test.shape)\n",
    "print(\"Shape y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Memisahkan kolom target dari data\n",
    "# target_col = train_df.filter(like = 'MSZoning_').columns\n",
    "\n",
    "# # Memastikan kolom target ada di dalam DataFrame sebelum mencoba memisahkannya\n",
    "# if target_col in train_df.columns:\n",
    "#     X_train = train_df.drop(columns = [target_col])\n",
    "#     y_train = train_df[target_col]\n",
    "\n",
    "# else:\n",
    "#     X_train = train_df\n",
    "#     y_train = None\n",
    "\n",
    "# if target_col in test_df.columns:\n",
    "#     X_test = test_df.drop(columns = [target_col])\n",
    "    \n",
    "# else:\n",
    "#     X_test = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat pipeline model\n",
    "pipelines = {\n",
    "    'RandomForest': Pipeline(steps = [\n",
    "        ('pca', PCA()),\n",
    "        ('classifier', RandomForestClassifier(random_state = 42))\n",
    "    ]),\n",
    "\n",
    "    'LogisticRegression': Pipeline(steps = [\n",
    "        ('pca', PCA()),\n",
    "        ('classifier', LogisticRegression(max_iter = 1000, random_state = 42))\n",
    "    ]),\n",
    "    \n",
    "    'SVC': Pipeline(steps = [\n",
    "        ('pca', PCA()),\n",
    "        ('classifier', SVC(random_state = 42))\n",
    "    ]),\n",
    "    \n",
    "    'GradientBoosting': Pipeline(steps = [\n",
    "        ('pca', PCA()),\n",
    "        ('classifier', GradientBoostingClassifier(random_state = 42))\n",
    "    ]),\n",
    "    \n",
    "    'XGBoost': Pipeline(steps = [\n",
    "        ('pca', PCA()),\n",
    "        ('classifier', XGBClassifier(use_label_encoder = False, eval_metric = 'mlogloss', random_state = 42))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid untuk GridSearchCV\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'pca__n_components': [0.90, 0.95, 0.99],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__bootstrap': [True, False],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'pca__n_components': [0.90, 0.95, 0.99],\n",
    "        'classifier__penalty': ['l1', 'l2', 'elasticnet'],  # Sesuaikan penalti yang valid\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__solver': ['liblinear', 'saga'],  # Solver yang mendukung kombinasi penalti\n",
    "        'classifier__max_iter': [100, 500, 1000],\n",
    "        'classifier__l1_ratio': [0.1, 0.5, 0.9],  # Hanya relevan untuk penalti 'elasticnet'\n",
    "    },\n",
    "    'SVC': {\n",
    "        'pca__n_components': [0.90, 0.95, 0.99],\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'classifier__degree': [2, 3, 4],\n",
    "        'classifier__gamma': ['scale', 'auto'],\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'pca__n_components': [0.90, 0.95, 0.99],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__subsample': [0.8, 0.9, 1.0],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'pca__n_components': [0.90, 0.95, 0.99],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__min_child_weight': [1, 3, 5],\n",
    "        'classifier__gamma': [0, 0.1, 0.5],\n",
    "        'classifier__subsample': [0.8, 0.9, 1.0],\n",
    "        'classifier__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Null and Infinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek NaN dan Inf di X_train\n",
    "print(f\"Jumlah NaN di X_train: {pd.isna(X_train).sum().sum()}\")\n",
    "X_train_numeric = X_train.select_dtypes(include = ['number'])\n",
    "print(f\"Jumlah Inf di X_train: {(np.isinf(X_train_numeric).sum().sum())} \\n\")\n",
    "\n",
    "# Cek NaN dan Inf di y_train\n",
    "print(f\"Jumlah NaN di y_train: {pd.isna(y_train).sum()}\")\n",
    "y_train_numeric = y_train\n",
    "print(f\"Jumlah Inf di y_train: {(np.isinf(y_train_numeric).sum().sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek NaN dan Inf di X_test\n",
    "print(f\"Jumlah NaN di X_test: {pd.isna(X_test).sum().sum()}\")\n",
    "X_test_numeric = X_test.select_dtypes(include = ['number'])\n",
    "print(f\"Jumlah Inf di X_test: {(np.isinf(X_test_numeric).sum().sum())} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "not_in_x_test = set(X_train.columns) - set(X_test.columns)\n",
    "not_in_x_train = set(X_test.columns) - set(X_train.columns)\n",
    "\n",
    "print(f\"Kolom yang ada di X_train tapi tidak ada di X_test: {not_in_x_test}\")\n",
    "print(f\"Kolom yang ada di X_test tapi tidak ada di X_train: {not_in_x_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "X_train = X_train.drop(columns = not_in_x_test)\n",
    "X_test = X_test.drop(columns = not_in_x_train)\n",
    "\n",
    "# \n",
    "not_in_x_test = set(X_train.columns) - set(X_test.columns)\n",
    "not_in_x_train = set(X_test.columns) - set(X_train.columns)\n",
    "\n",
    "print(f\"Kolom yang ada di X_train tapi tidak ada di X_test: {not_in_x_test}\")\n",
    "print(f\"Kolom yang ada di X_test tapi tidak ada di X_train: {not_in_x_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning and Model Selection\n",
    "best_models = {}\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\nTuning hyperparameters for {model_name}...\")\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv = 5, scoring = 'accuracy', n_jobs = -1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search\n",
    "    \n",
    "    print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Score for {model_name}: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation on Test Data\n",
    "for model_name, grid_search in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} on test data...\")\n",
    "\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    print(f\"Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
