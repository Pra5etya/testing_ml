{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mstats\n",
    "import warnings\n",
    "\n",
    "# data preparation\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "# data modeling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# data scoring\n",
    "\n",
    "\n",
    "# data tuning\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_round(x, pos): \n",
    "    if abs(x) >= 1e9: \n",
    "        return f'{x/1e9}B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6}M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3}K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x}'\n",
    "    \n",
    "def val_round(x):\n",
    "    if abs(x) >= 1e9:\n",
    "        return f'{x/1e9:.2f} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6:.2f} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3:.2f} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menonaktifkan traceback pada peringatan\n",
    "warnings.simplefilter(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor: float = 1.5):\n",
    "        self.factor = factor\n",
    "        self.bounds_ = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame | np.ndarray, y=None) -> \"OutlierRemover\":\n",
    "        X = self._convert_to_dataframe(X)\n",
    "        Q1 = X.quantile(0.25)\n",
    "        Q3 = X.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        self.bounds_ = {\n",
    "            col: (Q1[col] - self.factor * IQR[col], Q3[col] + self.factor * IQR[col])\n",
    "            for col in X.columns\n",
    "        }\n",
    "        for col in X.columns:\n",
    "            if IQR[col] == 0:\n",
    "                warnings.warn(f\"Column '{col}' has zero IQR; no outlier removal applied.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:\n",
    "        if self.bounds_ is None:\n",
    "            raise ValueError(\"This OutlierRemover instance is not fitted yet. Call 'fit' first.\")\n",
    "        X = self._convert_to_dataframe(X).copy()\n",
    "        for col, (low, high) in self.bounds_.items():\n",
    "            X[col] = X[col].clip(lower=low, upper=high)\n",
    "        return X.to_numpy()\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None) -> list:\n",
    "        if input_features is None:\n",
    "            return [f\"feature_{i}\" for i in range(len(self.bounds_))]\n",
    "        return input_features\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_dataframe(X: pd.DataFrame | np.ndarray) -> pd.DataFrame:\n",
    "        if isinstance(X, np.ndarray):\n",
    "            return pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            return X\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Input must be a pandas.DataFrame or numpy.ndarray, got \"\n",
    "                f\"{type(X).__name__}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk konversi tipe data\n",
    "def convert_object_columns_to_numeric(df):\n",
    "    for col in df.select_dtypes(include = ['object']).columns:  \n",
    "        try:\n",
    "            # Cek apakah semua nilai bisa dikonversi ke float\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "            \n",
    "            # Jika bisa, ubah ke int jika semua nilai adalah bilangan bulat\n",
    "            if all(df[col] % 1 == 0):  # Cek apakah semua nilai adalah bilangan bulat\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "        except ValueError:\n",
    "            pass  # Jika ada nilai non-angka, biarkan tetap object\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat data train dan test\n",
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all column\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "train_df = train_df.drop('Id', axis = 1)\n",
    "\n",
    "# convert object if all numeric\n",
    "train_df = convert_object_columns_to_numeric(train_df)\n",
    "\n",
    "# check duplicate general data\n",
    "print(f'Total General Duplicated: {train_df.duplicated().sum()} \\n')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null column\n",
    "null_numeric = []\n",
    "null_obj = []\n",
    "\n",
    "# \n",
    "null_columns = train_df.columns[train_df.isnull().sum() > 0]\n",
    "\n",
    "for col in null_columns:\n",
    "    if train_df[col].dtype in ['int', 'float']:\n",
    "        null_numeric.append(col)\n",
    "        \n",
    "    elif train_df[col].dtype == 'object':\n",
    "        null_obj.append(col)\n",
    "\n",
    "# \n",
    "print(\"Null Numeric:\", null_numeric)\n",
    "print(\"Null String:\", null_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "num_cols = []\n",
    "obj_cols = []\n",
    "\n",
    "for col in train_df:\n",
    "    if train_df[col].dtype in ['int', 'float']:\n",
    "        num_cols.append(col)\n",
    "        \n",
    "    elif train_df[col].dtype == 'object':\n",
    "        obj_cols.append(col)\n",
    "\n",
    "# \n",
    "print(\"Numeric Cols:\", num_cols)\n",
    "print(\"String Cols:\", obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns\n",
    "train_original = train_df.columns\n",
    "\n",
    "# Numeric Pipeline\n",
    "numerical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"mean\")), \n",
    "    (\"outlier_removal\", OutlierRemover(factor = 1.5))\n",
    "])\n",
    "\n",
    "# String Pipeline\n",
    "categorical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"most_frequent\"))\n",
    "])\n",
    "\n",
    "# ColumnTransformer untuk menggabungkan proses imputasi\n",
    "prep_stage_1 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_pipeline, num_cols), \n",
    "        (\"cat\", categorical_pipeline, obj_cols), \n",
    "    ], \n",
    "    remainder = \"drop\", \n",
    "    verbose_feature_names_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data menggunakan fit_transform pada tahap 1\n",
    "train_df = prep_stage_1.fit_transform(train_df)\n",
    "\n",
    "# Columns After: ubah kembali ke DataFrame dengan kolom dari prep_stage_1\n",
    "train_df = pd.DataFrame(train_df, columns = prep_stage_1.get_feature_names_out())\n",
    "\n",
    "# Hilangkan prefix (misalnya, \"num__\", \"cat__\", \"out__\")\n",
    "clean_columns = [col.split(\"__\", 1)[-1] for col in train_df.columns]\n",
    "train_df.columns = clean_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = train_df.isnull().sum()[train_df.isnull().sum() > 0]\n",
    "print(null_columns)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = convert_object_columns_to_numeric(train_df)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows before filtering\n",
    "print(f'Total Rows: {len(train_df)}')\n",
    "\n",
    "# Filter kolom numerik\n",
    "num_cols = train_df.select_dtypes(include = [\"number\"]).columns\n",
    "\n",
    "# Pipeline untuk outlier remover hanya pada kolom numerik\n",
    "outlier_pipeline = Pipeline(steps=[\n",
    "    (\"outlier_removal\", OutlierRemover(factor=1.5))\n",
    "])\n",
    "\n",
    "# Transformasi data hanya pada kolom numerik\n",
    "train_df[num_cols] = outlier_pipeline.fit_transform(train_df[num_cols])\n",
    "\n",
    "# Output jumlah baris setelah transformasi\n",
    "print(f'Total Rows: {len(train_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kolom untuk label encoding (kolom ordinal)\n",
    "encoding_set = {'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', \n",
    "                'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', \n",
    "                'FireplaceQu', 'GarageQual', 'GarageCond'}\n",
    "\n",
    "# Inisialisasi list untuk menyimpan kolom yang telah dikelompokkan\n",
    "train_ordinal_cols = []\n",
    "train_one_hot_cols = []\n",
    "train_numeric_cols = []\n",
    "\n",
    "# Mengelompokkan kolom berdasarkan tipe data\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype in ['int', 'float']:\n",
    "        train_numeric_cols.append(col)\n",
    "\n",
    "    elif train_df[col].dtype == 'object':\n",
    "        if col in encoding_set:\n",
    "            train_ordinal_cols.append(col)\n",
    "\n",
    "        else:\n",
    "            train_one_hot_cols.append(col)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Ordinal Encoding Columns:\", train_ordinal_cols)\n",
    "print(\"One-Hot Encoding Columns:\", train_one_hot_cols)\n",
    "print(\"Numeric Columns:\", train_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "test_df = test_df.drop('Id', axis = 1)\n",
    "\n",
    "# convert object if all numeric\n",
    "test_df = convert_object_columns_to_numeric(test_df)\n",
    "\n",
    "# check duplicate general data\n",
    "print(f'Total General Duplicated: {test_df.duplicated().sum()} \\n')\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null column\n",
    "null_numeric = []\n",
    "null_obj = []\n",
    "\n",
    "# \n",
    "null_columns = test_df.columns[test_df.isnull().sum() > 0]\n",
    "\n",
    "for col in null_columns:\n",
    "    if test_df[col].dtype in ['int', 'float']:\n",
    "        null_numeric.append(col)\n",
    "        \n",
    "    elif test_df[col].dtype == 'object':\n",
    "        null_obj.append(col)\n",
    "\n",
    "# \n",
    "print(\"Null Numeric:\", null_numeric)\n",
    "print(\"Null String:\", null_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "num_cols = []\n",
    "obj_cols = []\n",
    "\n",
    "\n",
    "for col in test_df:\n",
    "    if test_df[col].dtype in ['int', 'float']:\n",
    "        num_cols.append(col)\n",
    "        \n",
    "    elif test_df[col].dtype == 'object':\n",
    "        obj_cols.append(col)\n",
    "\n",
    "# \n",
    "print(\"Numeric Cols:\", num_cols)\n",
    "print(\"String Cols:\", obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns\n",
    "test_original = test_df.columns\n",
    "\n",
    "# Numeric Pipeline\n",
    "numerical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"mean\"))\n",
    "])\n",
    "\n",
    "# String Pipeline\n",
    "categorical_pipeline = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy = \"most_frequent\"))\n",
    "])\n",
    "\n",
    "# ColumnTransformer untuk menggabungkan proses imputasi\n",
    "prep_stage_1 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_pipeline, null_numeric), \n",
    "        (\"out\", OutlierRemover(factor = 1.5), num_cols),\n",
    "        (\"cat\", categorical_pipeline, null_obj), \n",
    "    ], remainder = \"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data menggunakan fit_transform pada tahap 1\n",
    "test_df = prep_stage_1.fit_transform(test_df)\n",
    "\n",
    "# implement original column\n",
    "test_df = pd.DataFrame(test_df, columns = test_original)\n",
    "\n",
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = test_df.isnull().sum()[test_df.isnull().sum() > 0]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kolom untuk label encoding (kolom ordinal)\n",
    "encoding_set = {'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', \n",
    "                'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', \n",
    "                'FireplaceQu', 'GarageQual', 'GarageCond'}\n",
    "\n",
    "# Inisialisasi list untuk menyimpan kolom yang telah dikelompokkan\n",
    "test_ordinal_cols = []\n",
    "test_one_hot_cols = []\n",
    "test_numeric_cols = []\n",
    "\n",
    "# Mengelompokkan kolom berdasarkan tipe data\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype in ['int', 'float']:\n",
    "        test_numeric_cols.append(col)\n",
    "\n",
    "    elif train_df[col].dtype == 'object':\n",
    "        if col in encoding_set:\n",
    "            test_ordinal_cols.append(col)\n",
    "\n",
    "        else:\n",
    "            test_one_hot_cols.append(col)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Ordinal Encoding Columns:\", test_ordinal_cols)\n",
    "print(\"One-Hot Encoding Columns:\", test_one_hot_cols)\n",
    "print(\"Numeric Columns:\", test_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifikasi kolom-kolom yang ada di train dan test\n",
    "ordinal_encoding_cols = list(set(train_ordinal_cols) & set(test_ordinal_cols))\n",
    "one_hot_encoding_cols = list(set(train_ordinal_cols) & set(test_ordinal_cols))\n",
    "numeric_cols = list(set(train_ordinal_cols) & set(test_ordinal_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan kolom target dari data\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "# Memastikan kolom target ada di dalam DataFrame sebelum mencoba memisahkannya\n",
    "if target_col in train_df.columns:\n",
    "    X_train = train_df.drop(columns = [target_col])\n",
    "    y_train = train_df[target_col]\n",
    "\n",
    "else:\n",
    "    X_train = train_df  \n",
    "    y_train = None  \n",
    "\n",
    "if target_col in test_df.columns:\n",
    "    X_test = test_df.drop(columns = [target_col])\n",
    "    \n",
    "else:\n",
    "    X_test = test_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)\n",
    "ordinal_transformer = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)\n",
    "\n",
    "prep_stage_2 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_transformer, numeric_cols), \n",
    "        (\"cat\", categorical_transformer, one_hot_encoding_cols), \n",
    "        (\"ord\", ordinal_transformer, ordinal_encoding_cols)\n",
    "    ], remainder = \"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data menggunakan fit_transform pada tahap 1\n",
    "train_check = prep_stage_2.fit_transform(train_df)\n",
    "\n",
    "# implement original column\n",
    "train_check = pd.DataFrame(train_df, columns = train_original)\n",
    "\n",
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = train_check.isnull().sum()[train_check.isnull().sum() > 0]\n",
    "print(f'Train Stage 2 Check: {null_columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data menggunakan fit_transform pada tahap 1\n",
    "test_check = prep_stage_2.fit_transform(test_df)\n",
    "\n",
    "# implement original column\n",
    "test_check = pd.DataFrame(test_df, columns = test_original)\n",
    "\n",
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = test_check.isnull().sum()[test_check.isnull().sum() > 0]\n",
    "print(f'Test Stage 2 Check: {null_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat pipeline yang menggabungkan preprocessing dengan model\n",
    "model_pipeline = Pipeline(steps = [\n",
    "    ('preprocessor', prep_stage_2),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
